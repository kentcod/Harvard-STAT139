---
title: "Problem Set 2"
author: "STAT 139 (Fall 2024) Teaching Team"
date: "Due Friday, September 20, 2024 at 11:59pm"
fontsize: 11pt
geometry: margin=1in
output:
  pdf_document:
    includes:
    fig_width: 5
    fig_height: 3.5
urlcolor: blue
---





\begin{small} 
		
\textbf{Problem set policies.} \textit{Please provide concise, clear answers for each question while making sure to fully explain your reasoning. For problems asking for calculations, show your work in addition to clearly indicating the final answer. For problems involving \texttt{R}, be sure to include the code and output in your solution.}

\textit{Please submit the PDF of your knit solutions to Gradescope and be sure to assign which pages of your solution correspond to each problem. Make sure that the PDF is fully readable to the graders; e.g., make sure that lines don't run off the page margin.}

\textit{We encourage you to discuss problems with other students (and, of course, with the teaching team), but you must write your final answer in your own words. Solutions prepared ``in committee'' are not acceptable. If you do collaborate with classmates on a problem, please list your collaborators on your solution. Be aware that simply copying answers found online, whether human-generated or machine-generated, is a violation of the Honor Code.}
		
\end{small}


\clearpage



### Question 1: Creating our Dataset

We will create our own dataset to try to replicate the analyses we did in class with the human height data (the application in which the method of regression was developed). Start by filling out the questionnaire here:

\url{https://forms.gle/VQJJsNz4Vjs9RFAk6}

\vspace{1em}

### Question 2: Some Algebra

Let $x_1, x_2, \dots, x_n$ be any numbers and define $\bar{x}=\frac{1}{n}\sum_{i=1}^{n} x_i$  and $s^2 = \frac{1}{n-1}\sum_{i=1}^{n} (x_i - \bar{x})^2$. Prove the following:

(a) $\text{argmin}_a \sum_{i=1}^{n} (x_i - a)^2 = \bar{x}$

$\text{argmin}_a \sum_{i=1}^{n} (x_i - a)^2$ when $\frac{d}{dx}(\sum_{i=1}^{n}(x_i-a)^2=0$

$$
2\sum_{i=1}^{n}(x_i-a)*-1= 0
\\
-2(\sum_{i=1}^{n}x_i-an) = 0
\\
\frac{1}{n}\sum_{i=1}^{n}x_i = a
$$

(b) $(n-1)s^2 = \sum_{i=1}^{n} x_i^2 - n \bar{x}^2$

$$
(n-1) \frac{1}{n-1}\sum_{i=1}^{n} (x_i - \bar{x})^2 = \sum_{i=1}^{n} x_i^2 - {n}\bar{x}^2
$$
$$
\sum_{i=1}^{n}x_i^2-\sum_{i=1}^{n}(2x_i\bar{x})+\sum_{i=1}^{n}\bar{x}^2 = \sum_{i=1}^{n}x_i^2-n\bar{x}^2
$$
$$
n * -2\bar{x}*\bar{x} + n\bar{x}^2 = -n\bar{x}^2
$$
$$
  -n\bar{x}^2 = -n\bar{x}^2
$$
(c) $\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) = \sum_{i=1}^{n} x_i y_i -n \bar{x} \bar{y}$

$$
\sum_{i=1}^{n} (x_iy_i - x_i\bar{y} - y_i\bar{x}+\bar{x}\bar{y}) = \sum_{i=1}^{n}x_iy_i - n\bar{x}\bar{y}
$$
$$
-\bar{y}\sum_{i=1}^{n}x_i-\bar{x}\sum_{i=1}^{n}y_i+\sum_{i=1}^{n}\bar{x}\bar{y} = -n\bar{x}\bar{y}
$$
$$
n*(-\bar{y}\bar{x})+n*(-\bar{y}\bar{x}) + n\bar{y}\bar{x} = -n\bar{y}\bar{x}
$$

$$
-n\bar{y}\bar{x} = -n\bar{y}\bar{x}
$$

\vspace{1em}

### Question 3: Deriving the OLS Estimates

Assume the following population regression model:

$$Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i \hspace{3em} i=1,\dots n$$

where the ELIH assumptions we learned in class hold. Derive the Ordinary Least Squares (OLS) estimates of $\beta_0$ and $\beta_1$ using calculus, as described in Lecture 3. That is, show:

$$\hat{\beta_0} = \bar{Y} - \hat{\beta}_1\bar{x} \hspace{0.2in} \hat{\beta_1} = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(Y_i-\bar{Y})}{\sum_{i=1}^{n} (x_i-\bar{x})^2}$$
$$
J(\hat{\beta_0},\hat{\beta_1}) = \hat{\beta_0} = \bar{Y} - \hat{\beta}_1\bar{x}
$$
$$
\frac{\partial J(B_0, B_1)}{\partial B_0} = \sum_{i=1}^{n}2[Y_i-\hat{\beta_0}+\hat{\beta_1}x_i]*(-1)
$$
set derivative to 0
$$
0 = -2\sum_{i=1}^{n}(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)
$$
$$
0 = n\bar{y}-n\hat{\beta_0}-n\hat{\beta_1}\bar{x}
$$
$$
\hat{\beta_0}= \bar{y}-\hat{\beta_1}\bar{x}
$$
$$
\frac{\partial J(B_0, B_1)}{\partial B_1} =  \sum_{i=1}^{n}2[Y_i-\hat{\beta_0}-\hat{\beta_1}x_i]*x_i
$$
$$
0 =  2x_i[\sum_{i=1}^{n}Y_i-\sum_{i=1}^{n}\hat{\beta_0}-\sum_{i=1}^{n}\hat{\beta_1}x_i]
$$
$$
0 =  x_i\sum_{i=1}^{n}Y_i-x_i\sum_{i=1}^{n}\hat{\beta_0}-x_i\sum_{i=1}^{n}\hat{\beta_1}x_i
$$
$$
x_i\sum_{i=1}^{n}\hat{\beta_1}x_i = x_i\sum_{i=1}^{n}Y_i-x_i\sum_{i=1}^{n}\hat{\beta_0}
$$
substitute B_0 in terms of B_1
$$
\hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}Y_ix_i-[\bar{y}-\hat{\beta_1}\bar{x}]\sum_{i=1}^{n}x_i
$$
$$
\hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}Y_ix_i-[\sum_{i=1}^{n}x_i\bar{y}-\hat{\beta_1}\bar{x}x_i]
$$
The facts you proved in Question 2 will help you.

$$
\hat{\beta_1}\sum_{i=1}^{n}x_i^2 = [\sum_{i=1}^{n}Y_ix_i-n\bar{x}\bar{y}]-\sum_{i=1}^{n}\hat{\beta_1}\bar{x}x_i
$$
from 2c and 2b

$$
\hat{\beta_1}\sum_{i=1}^{n}x_i^2 -\hat{\beta_1}\bar{x}^2n = [\sum_{i=1}^{n}(Y_i-\bar{Y})(x_i-\bar{x})]
$$
$$
\hat{\beta_1}(\sum_{i=1}^{n}x_i^2 -\sum_{i=1}^{n}\bar{x}^2) = [\sum_{i=1}^{n}(Y_i-\bar{Y})(x_i-\bar{x})]
$$
$$
\hat{\beta_1} = \frac{\sum_{i=1}^{n}(Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i^2 -\bar{x}^2)}
$$
\vspace{1em}

### Problem 4. Correlation of OLS Estimates

Assuming the model in Problem 3 holds:

(a) Show that $\bar{Y}$ and $\hat{\beta}_1$ are independent. This will help you in part (b).

solve by showing that the covariance between $\bar{Y}$ and $\hat{\beta}_1$ is 0

$$
Cov(\bar{Y},\hat{\beta_1}) = Cov(\frac{1}{n}\sum_{i=1}^{n}y_i, \hat{\beta_1} = \frac{\sum_{i=1}^{n}(Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i^2 -\bar{x}^2)})
\\
=\frac{1}{n* {\sum_{i=1}^{n}(x_i^2 -\bar{x}^2)}} * \sum_{i=1}^{n}y_i \sum_{i=1}^{n}(Y_i-\bar{Y})(x_i-\bar{x})
$$

$$
=\frac{1}{n* {\sum_{i=1}^{n}(x_i^2 -\bar{x}^2)}} * \sum_{i=1}^{n}(x_i-\bar{x}) Cov(Y_i,Y_i)
\\
=\frac{1}{n* {\sum_{i=1}^{n}(x_i^2 -\bar{x}^2)}} * \sum_{i=1}^{n}(x_i-\bar{x}) * \sigma^2
\\
= \frac{1}{n} \sum_{i=1}^{n}(x_i-\bar{x})
= 0
$$
sum of deviations is 0 => Covariance is 0 => $\bar{Y}$ and $\hat{\beta_1}$ are independent.

(b) Derive the covariance and correlation between $\hat{\beta}_1$ and $\hat{\beta}_0$.


$$
Cov(\hat{\beta_1}, \hat{\beta_0}) = Cov(\hat{\beta_1}, \bar{Y}-\hat{\beta_1}\bar{x})
\\
= Cov(\hat{\beta_1}, \bar{Y})-Cov(\hat{\beta_1},\hat{\beta_1})\bar{x}
\\
= -Var(\hat{\beta_1}) \bar{x}
\\
Corr(\hat{\beta_1}, \hat{\beta_0}) = \frac{ Cov(\hat{\beta_1},\hat{\beta_0})}{\sqrt{Var(\hat{\beta_1})Var(\hat{\beta_0})}}
\\
$$

$$
= - \frac{\frac{\sigma^2}{\sum(x_i-\bar{x})^2}*\bar{x}}{\sqrt{Var(\hat{\beta_1})Var(\hat{\beta_0})}}
\\
Var(\hat{\beta_0}) = Var(\bar{Y})+\bar{x}^2Var(\beta_1)
\\
$$

$$
Var(\hat{\beta_0}) = \sigma/n + \bar{x}Var(\beta_1)
$$
$$
\frac{-\bar{x}Var(\hat\beta_1)}{\sqrt{(\sigma^2+\bar{x}Var(\hat{\beta_1}))(Var\hat{\beta_1})}}
$$

(c) In terms of $\bar{x}$, when will this correlation be positive?  When will it be negative?  In 1-2 sentences, justify why this makes sense if $\bar{x}>0$ (think where the scatterplot and regression line lies on the coordinate system). 

If the mean of $x_i$ is greater than 0, the slope increases, thus decreasing the overall correlation between $\hat{\beta_1}$ and $\hat{\beta_2}$. 

(d) Show that the variance of $\hat{\mu}_{Y|X=x_0}$ is $\sigma^2\left[\frac{1}{n}+\frac{(x_0-\bar{x})^2}{S_{xx}}\right]$  
<!-- (which is used in creating the confidence intervals and prediction intervals around the regression line at a particular $X_0$). -->

$$
\hat{\mu}_{Y|X=x_0} = Var(\bar{Y}-\hat{\beta_1}(x_0-\bar{x}))
\\
= Var(\bar{Y})+ (x_0-\bar{x})^2Var(\hat{\beta_1})+2(x_0-\bar{x})Cov(\bar{y},\hat{\beta_1})
$$
$$
= \sigma/n +  (x_0-\bar{x})^2 * \frac{\sigma^2}{\sum_{i=1}^{n}(x_i-\bar{x}^2)} + 0
\\
= \sigma^2\left[\frac{1}{n}+\frac{(x_0-\bar{x})^2}{S_{xx}}\right]
$$

\vspace{1em}

### Question 5: Fitting the Model

All empirical work begins with some data "cleaning", including ensuring the data are in the right form. James will do this for you this time. After he cleans the data you provided, he will provide you with a dataset with these three variables:

- \texttt{studentheight}: your heights (in inches, to the nearest half inch)

- \texttt{maternalheight}: the height of your mothers (in inches, to the nearest half inch)

- \texttt{paternalheight}: the height of your fathers (in inches, to the nearest half inch)


(a) Next, it's important to conduct exploratory data analysis (EDA) to ensure the integrity of your data. This includes summarizing your variables, including the extent of missingness, checking for outliers and inconsistencies, and potentially addressing any data entry errors. Provide an appropriate EDA (e.g., appropriate figures and/or a table), and provide commentary. Also, justify any decisions you make about how you choose to handle any suspect data in your analysis.

```{r}
# read in data
library(readr)
heights_139 <- read_csv("heights_139.csv")

#output summary
summary(heights_139)
```
class is character, so will have to cast to numeric

First, convert inches, and cm to inches
```{r}
# find all problem indices with apostrophes or inches written out
problem_idx = sapply(heights_139, function(column) nchar(as.character(column)) > 3 | grepl("'", as.character(column)))

heights_139[problem_idx]
```
```{r}

# Function to convert heights to inches as numeric format
convert <- function(h) {
  h <- trimws(tolower(h))
  
  # find 'feet' and 'inches' to convert
  if (grepl("'", h)) {
    parts <- strsplit(h, "'")[[1]]
    f <- as.numeric(parts[1])
    inc <- as.numeric(gsub(" inches", "", parts[2]))
    return(f * 12 + inc)
  }
  # just get rid of 'inches'
  if (grepl("inches", h)) {
    return(as.numeric(gsub(" inches", "", h)))
  # otherwise can be casted to numeric
  }
  if (grepl("^[0-9.]+$", h)) {
    return(as.numeric(h))
  #NA if none of previous conversions work
  }
  return(NA)
}
```

```{r}
#add values to dataframe and get rid of rows with NA
heights_clean <- as.data.frame(na.omit(apply(heights_139, c(1, 2), convert)))
```


```{r}
summary(heights_clean)
```
summary now shows statistics since class is numeric

```{r}
#show summary df plot
plot(heights_clean,
     col = 'coral',
     pch = 2)
```
clear outlier
```{r}
# remove outliers for nonsensical heights
idx = which(heights_clean$studentheight>100 | heights_clean$studentheight < 30)
heights_clean = heights_clean[-idx,] #all rows besides idx of outlier, all columns
```

```{r}
plot(heights_clean,
     col = 'coral',
     pch = 3)
```
plots now readable with some correlation!

```{r}
#distributions for each variable
hist(heights_clean$studentheight,
     col = 'blue3',
     breaks = 8)
```
```{r}
hist(heights_clean$paternalheight,
     col = 'red3',
     breaks = 8)
```
```{r}
hist(heights_clean$maternalheight,
     col = 'green3',
     breaks = 8)
```


(b) Create the following new variables in your dataset:

- \texttt{midparentheight} = \texttt{paternalheight} + 1.08$\times$\texttt{maternalheight}

```{r}
heights_clean$midparentheight = (heights_clean$paternalheight+1.08*heights_clean$maternalheight)/2
```


- \texttt{tallparents} = 1 if \texttt{midparentheight} is greater than or equal to the median of \texttt{midparentheight} and 0 if not. That is, create an *indicator variable* of whether \texttt{midparentheight} is greater than or equal to the median.
```{r}
# create T/F vector of rows and coerce to numeric

heights_clean$tallparents <- as.numeric( heights_clean$midparentheight >= median(heights_clean$midparentheight))
```


(c)  Fit a simple linear regression in \textsf{R}, with \texttt{studentheight} as your dependent variable, and \texttt{tallparents} as your independent variable. Interpret the slope coefficient from your model.

```{r}
lm5c <- lm(heights_clean$studentheight ~ heights_clean$tallparents)
summary(lm5c)
```
the $\hat{\beta_1}$ is positive, which means that the model predicts that students with tall parents (a one-unit increase from 0 to 1), as denoted by the binary column, will be about 3 inches taller than students without tall parents in our STAT 139 class

(d) Test whether there is sufficient evidence to indicate a true mean difference in heights of the children of taller versus shorter parents, using a two sample $t$-test in \textsf{R} at the $\alpha=0.05$ level of significance. Make sure to formally state your hypothesis, report your test statistic and interpret the associated p-value.

### Alternate Hypothesis:
there is a true mean difference in heights of the children of taller versus shorter parents

```{r}
tall <- heights_clean$studentheight[heights_clean$tallparents == 1]
short <- heights_clean$studentheight[heights_clean$tallparents == 0]
t.test(tall, short, )
```
the test statistic is 3.0268
the p-value is 0.003805, which rejects the null at an $\alpha$ of 0.05, which means that there is roughly a 0.3% chance that these two means are different due to random variation/ noise.

(e) Comment on the consistencies and/or inconsistencies between the output of \texttt{lm()} in part (c) and the output of \texttt{t.test()} in part (d). Is there a connection between the estimate from \texttt{lm()} and the test statistic from \texttt{t.test()}? What about the inference from the two approaches? You don't need to derive anything here, just comment on the outputs.

The outputs are similar in both the p-value and the signal coefficient. There seems to be a connection. Because the t-values are the same, the binary predictor has the same magnitude in both the linear regression and the t-test.